{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Langchain x Phoenix\n",
    "\n",
    "The following notebooks showcases the power of Langchain and Phoenix combined. \n",
    "\n",
    "I am quite passionate about the Trust and Safety domain, and thus thought there would be lots of utility in scraping and building a RAG around this website: https://features.integrityinstitute.org/. I call it the Integrity RAG. For the sake of simplicity, I am focusing on a subset of online harms.\n",
    "\n",
    "## Scope of Notebook\n",
    "\n",
    "The order of steps is the following:\n",
    "1. Setup OpenAI, Phoenix, and Langchain\n",
    "2. Build Integrity RAG.\n",
    "3. Generate LLM Based Questions (substituting for user questions)\n",
    "4. Generate RAG-based Answers \n",
    "5. Generate LLM Based Evaluations (substituting for user based)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Phoenix\n",
    "\n",
    "It's quite simple actually to setup. Connect phoenix using the LangChainInstrumentor which acts like magic.\n",
    "\n",
    "\n",
    "Side Note: This code was hard to find in the onboarding flow. Llama-index was much easier to find"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Attempting to instrument while already instrumented\n",
      "Existing running Phoenix instance detected! Shutting it down and starting a new instance...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üåç To view the Phoenix app in your browser, visit http://localhost:6006/\n",
      "üì∫ To view the Phoenix app in a notebook, run `px.active_session().view()`\n",
      "üìñ For more information on how to use Phoenix, check out https://docs.arize.com/phoenix\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<phoenix.session.session.ThreadSession at 0x2aab38810>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import nest_asyncio\n",
    "import phoenix as px\n",
    "from phoenix.trace.langchain import LangChainInstrumentor\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"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\"\n",
    "nest_asyncio.apply()\n",
    "LangChainInstrumentor().instrument()\n",
    "px.launch_app()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup the RAG\n",
    "\n",
    "Sets up a simple RAG using a RecursiveUrlLoader, simple length 1000 chunking, k=6. No experimentation was done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as Soup\n",
    "from langchain_community.document_loaders import RecursiveUrlLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.vectorstores.utils import filter_complex_metadata\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain import hub\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "URL = \"https://features.integrityinstitute.org/\" \n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "loader = RecursiveUrlLoader(url=URL, max_depth=2, extractor=lambda x: Soup(x, \"html.parser\").text)\n",
    "docs = loader.load()\n",
    "\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, chunk_overlap=200, add_start_index=True\n",
    ")\n",
    "all_chunks = splitter.split_documents(docs)\n",
    "filtered_chunks = filter_complex_metadata(all_chunks)\n",
    "\n",
    "vector_store = Chroma.from_documents(documents=filtered_chunks, embedding=OpenAIEmbeddings())\n",
    "retriever = vector_store.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 6})\n",
    "\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate LLM Based Questions\n",
    "\n",
    "This section generates questions using gpt-4 for our RAG to simulate dummy data. In practice, ideally we have real data for our RAG application.\n",
    "\n",
    "That being said, the llm_generate function should allow us a flag to easily concat the input instead of having to do so manually. Ideally, the llm_generate does all the work for us! #TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llm_generate |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 144/144 (100.0%) | ‚è≥ 01:12<00:00 |  1.98it/s\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from phoenix.experimental.evals import OpenAIModel, llm_generate\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def output_parser(response: str, index: int):\n",
    "    try:\n",
    "        return json.loads(response)\n",
    "    except json.JSONDecodeError as e:\n",
    "        return {\"__error__\": str(e)}\n",
    "\n",
    "generate_questions_template = \"\"\"\\\n",
    "Context information is below.\n",
    "\n",
    "---------------------\n",
    "{text}\n",
    "---------------------\n",
    "\n",
    "Given the context information and not prior knowledge.\n",
    "generate only questions based on the below query.\n",
    "\n",
    "You are a Teacher/ Professor. Your task is to setup \\\n",
    "2 questions for an upcoming \\\n",
    "quiz/examination. The questions should be diverse in nature \\\n",
    "across the document. Restrict the questions to the \\\n",
    "context information provided.\"\n",
    "\n",
    "Output the questions in JSON format with the keys question_1, question_2.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "chunks_df = pd.DataFrame({\"text\": [doc.page_content for doc in filtered_chunks]})\n",
    "\n",
    "questions_df = llm_generate(\n",
    "    dataframe=chunks_df,\n",
    "    template=generate_questions_template,\n",
    "    model=OpenAIModel(\n",
    "        model_name=\"gpt-3.5-turbo\",\n",
    "    ),\n",
    "    output_parser=output_parser,\n",
    "    concurrency=4,\n",
    ")\n",
    "\n",
    "\n",
    "questions_chunks_df = pd.concat([questions_df, chunks_df], axis=1)\n",
    "questions_chunks_df = questions_chunks_df.melt(\n",
    "    id_vars=[\"text\"], value_name=\"question\"\n",
    ").drop(\"variable\", axis=1)\n",
    "questions_chunks_df = questions_chunks_df[\n",
    "    questions_chunks_df[\"question\"].notnull()\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer Questions Based on RAG\n",
    "\n",
    "This section answers the sample questions using our RAG. \\\n",
    "Note: we don't have to store the results, since Phoenix takes care of that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 288/288 [09:54<00:00,  2.06s/it]\n"
     ]
    }
   ],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "total_tasks = len(questions_chunks_df[\"question\"])\n",
    "progress_bar = tqdm(total=total_tasks, desc=\"Processing\")\n",
    "\n",
    "def invoke_chain_with_print(question):\n",
    "    result = rag_chain.invoke(question)\n",
    "    progress_bar.update(1)\n",
    "    return result\n",
    "    \n",
    "\n",
    "with ThreadPoolExecutor(max_workers=2) as executor:\n",
    "    results = executor.map(invoke_chain_with_print, questions_chunks_df[\"question\"])\n",
    "    \n",
    "progress_bar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "This section evaluates the RAG using GPT.\n",
    "\n",
    "We can already start to see the inefficiencies here. This is quite hard to use. The pain is quite apparent, we have to:\n",
    "\n",
    "1. We have to define a Relevance Evaluator\n",
    "2. Massage the dataframe\n",
    "3. compute the nDCG\n",
    "4. precision\n",
    "5. hit rate ourselves\n",
    "\n",
    "\n",
    "Generally speaking almost all this code, can be put into one \"evaluate and put on phoenix\". The API is not declarative at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from phoenix.experimental.evals import (\n",
    "    RelevanceEvaluator,\n",
    "    run_evals,\n",
    ")\n",
    "from phoenix.session.evaluation import get_retrieved_documents\n",
    "import numpy as np\n",
    "from sklearn.metrics import ndcg_score\n",
    "\n",
    "\n",
    "retrieved_documents_df = get_retrieved_documents(px.Client())\n",
    "relevance_evaluator = RelevanceEvaluator(OpenAIModel(model_name=\"gpt-4-turbo-preview\"))\n",
    "\n",
    "retrieved_documents_relevance_df = run_evals(\n",
    "    evaluators=[relevance_evaluator],\n",
    "    dataframe=retrieved_documents_df,\n",
    "    provide_explanation=True,\n",
    "    concurrency=20,\n",
    ")[0]\n",
    "\n",
    "documents_with_relevance_df = pd.concat(\n",
    "    [retrieved_documents_df, retrieved_documents_relevance_df.add_prefix(\"eval_\")], axis=1\n",
    ")\n",
    "\n",
    "def _compute_ndcg(df, n):\n",
    "    n = max(len(df), 2)\n",
    "    eval_scores = np.zeros(n)\n",
    "    doc_scores = np.zeros(n)\n",
    "    eval_scores[: len(df)] = df.eval_scores\n",
    "    doc_scores[: len(df)] = df.document_scores\n",
    "    \n",
    "    try:\n",
    "        return ndcg_score([eval_scores], [doc_scores], k=k)\n",
    "    except ValueError:\n",
    "        return np.nan\n",
    "\n",
    "ndcg_at_2 = pd.DataFrame(\n",
    "    {\"score\": documents_with_relevance_df.groupby(\"context.span_id\").apply(_compute_ndcg, k=2)}\n",
    ")\n",
    "\n",
    "precision_at_2 = pd.DataFrame(\n",
    "    {\n",
    "        \"score\": documents_with_relevance_df.groupby(\"context.span_id\").apply(\n",
    "            lambda x: x.eval_score[:2].sum(skipna=False) / 2\n",
    "        )\n",
    "    }\n",
    ")\n",
    "\n",
    "hit = pd.DataFrame(\n",
    "    {\n",
    "        \"hit\": documents_with_relevance_df.groupby(\"context.span_id\").apply(\n",
    "            lambda x: x.eval_score[:2].sum(skipna=False) > 0\n",
    "        )\n",
    "    }\n",
    ")\n",
    "\n",
    "retrievals_df = px.Client().get_spans_dataframe(\"span_kind == 'RETRIEVER'\")\n",
    "rag_evaluation_dataframe = pd.concat(\n",
    "    [\n",
    "        retrievals_df[\"attributes.input.value\"],\n",
    "        ndcg_at_2.add_prefix(\"ncdg@2_\"),\n",
    "        precision_at_2.add_prefix(\"precision@2_\"),\n",
    "        hit,\n",
    "    ],\n",
    "    axis=1,\n",
    ")\n",
    "from phoenix.trace import DocumentEvaluations, SpanEvaluations\n",
    "\n",
    "px.log_evaluations(\n",
    "    SpanEvaluations(dataframe=ndcg_at_2, eval_name=\"ndcg@2\"),\n",
    "    SpanEvaluations(dataframe=precision_at_2, eval_name=\"precision@2\"),\n",
    "    DocumentEvaluations(dataframe=retrieved_documents_relevance_df, eval_name=\"relevance\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Appendix\n",
    "\n",
    "If interested you can peek at some of the data structures below. The goal was to remove this from view, as it was obstructing the natural flow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'phoenix' has no attribute 'active_session'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mphoenix\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msession\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mevaluation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_retrieved_documents\n\u001b[0;32m----> 4\u001b[0m retrieved_documents_df \u001b[38;5;241m=\u001b[39m get_retrieved_documents(\u001b[43mpx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mClient\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/work/simulacra/.venv/lib/python3.11/site-packages/phoenix/session/client.py:51\u001b[0m, in \u001b[0;36mClient.__init__\u001b[0;34m(self, endpoint, use_active_session_if_available)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_session \u001b[38;5;241m=\u001b[39m Session()\n\u001b[1;32m     50\u001b[0m weakref\u001b[38;5;241m.\u001b[39mfinalize(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_session\u001b[38;5;241m.\u001b[39mclose)\n\u001b[0;32m---> 51\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_use_active_session_if_available \u001b[38;5;129;01mand\u001b[39;00m \u001b[43mpx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactive_session\u001b[49m()):\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_warn_if_phoenix_is_not_running()\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'phoenix' has no attribute 'active_session'"
     ]
    }
   ],
   "source": [
    "from phoenix.session.evaluation import get_retrieved_documents\n",
    "\n",
    "\n",
    "retrieved_documents_df = get_retrieved_documents(px.Client())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<phoenix.session.session.ThreadSession at 0x2aab38810>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
