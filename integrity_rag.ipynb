{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Langchain x Phoenix\n",
    "\n",
    "The following notebooks showcases the power of Langchain and Phoenix combined. \n",
    "\n",
    "I am quite passionate about the Trust and Safety domain, and thus thought there would be lots of utility in scraping and building a RAG around this website: https://features.integrityinstitute.org/. I call it the Integrity RAG. For the sake of simplicity, I am focusing on a subset of online harms.\n",
    "\n",
    "## Scope of Notebook\n",
    "\n",
    "The order of steps is the following:\n",
    "1. Setup OpenAI, Phoenix, and Langchain\n",
    "2. Build Integrity RAG.\n",
    "3. Generate LLM Based Questions (substituting for user questions)\n",
    "4. Generate RAG-based Answers \n",
    "5. Generate LLM Based Evaluations (substituting for user based)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Phoenix\n",
    "\n",
    "It's quite simple actually to setup. Connect phoenix using the LangChainInstrumentor which acts like magic.\n",
    "\n",
    "\n",
    "Side Note: This code was hard to find in the onboarding flow. Llama-index was much easier to find"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nest_asyncio\n",
    "import phoenix as px\n",
    "from phoenix.trace.langchain import LangChainInstrumentor\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"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\"\n",
    "nest_asyncio.apply()\n",
    "LangChainInstrumentor().instrument()\n",
    "px.launch_app()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup the RAG\n",
    "\n",
    "Sets up a simple RAG using a RecursiveUrlLoader, simple length 1000 chunking, k=6. No experimentation was done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as Soup\n",
    "from langchain_community.document_loaders import RecursiveUrlLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.vectorstores.utils import filter_complex_metadata\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain import hub\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "URL = \"https://features.integrityinstitute.org/\" \n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "loader = RecursiveUrlLoader(url=URL, max_depth=2, extractor=lambda x: Soup(x, \"html.parser\").text)\n",
    "docs = loader.load()\n",
    "\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, chunk_overlap=200, add_start_index=True\n",
    ")\n",
    "all_chunks = splitter.split_documents(docs)\n",
    "filtered_chunks = filter_complex_metadata(all_chunks)\n",
    "\n",
    "vector_store = Chroma.from_documents(documents=filtered_chunks, embedding=OpenAIEmbeddings())\n",
    "retriever = vector_store.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 6})\n",
    "\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Have to generate some llm generated questions to simulate users. \n",
    "\n",
    "#  Next is to add in fake data\n",
    "#  some feedback items I have, it's hard to find how to incorporate pheonix with langchain\n",
    "#  once you do, the api for pheonix is not obvious\n",
    "#  why doesn't pheonix UI have an easy way to make LLM data and use it to eval\n",
    "#  why doesn't pheonix UI have like idk, buttons to check eval.\n",
    "#  too much pandas here, and too many things to do. I had to llm to dummy data, and then i had to get another llm to actually do the evaluate\n",
    "#  the onboarding is crazy\n",
    "#  give open source onboarding ideas\n",
    "#  and give non-open source onboarding dieas\n",
    "# why not open source models as well\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate LLM Based Questions\n",
    "\n",
    "This section generates questions using gpt-4 for our RAG to simulate dummy data. In practice, ideally we have real data for our RAG application.\n",
    "\n",
    "That being said, the llm_generate function should allow us a flag to easily concat the input instead of having to do so manually. Ideally, the llm_generate does all the work for us! #TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from phoenix.experimental.evals import OpenAIModel, llm_generate\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def output_parser(response: str, index: int):\n",
    "    try:\n",
    "        return json.loads(response)\n",
    "    except json.JSONDecodeError as e:\n",
    "        return {\"__error__\": str(e)}\n",
    "\n",
    "generate_questions_template = \"\"\"\\\n",
    "Context information is below.\n",
    "\n",
    "---------------------\n",
    "{text}\n",
    "---------------------\n",
    "\n",
    "Given the context information and not prior knowledge.\n",
    "generate only questions based on the below query.\n",
    "\n",
    "You are a Teacher/ Professor. Your task is to setup \\\n",
    "3 questions for an upcoming \\\n",
    "quiz/examination. The questions should be diverse in nature \\\n",
    "across the document. Restrict the questions to the \\\n",
    "context information provided.\"\n",
    "\n",
    "Output the questions in JSON format with the keys question_1, question_2, question_3.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "chunks_df = pd.DataFrame({\"text\": [doc.page_content for doc in filtered_chunks]})\n",
    "\n",
    "questions_df = llm_generate(\n",
    "    dataframe=chunks_df,\n",
    "    template=generate_questions_template,\n",
    "    model=OpenAIModel(\n",
    "        model_name=\"gpt-3.5-turbo\",\n",
    "    ),\n",
    "    output_parser=output_parser,\n",
    "    concurrency=20,\n",
    ")\n",
    "\n",
    "\n",
    "questions_chunks_df = pd.concat([questions_df, chunks_df], axis=1)\n",
    "questions_chunks_df = questions_chunks_df.melt(\n",
    "    id_vars=[\"text\"], value_name=\"question\"\n",
    ").drop(\"variable\", axis=1)\n",
    "questions_chunks_df = questions_chunks_df[\n",
    "    questions_chunks_df[\"question\"].notnull()\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer Questions Based on RAG\n",
    "\n",
    "This section answers the sample questions using our RAG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=8) as executor:\n",
    "    _ = list(executor.map(rag_chain.invoke, questions_chunks_df[\"question\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "This section evaluates the RAG using GPT.\n",
    "\n",
    "We can already start to see the inefficiencies here. This is quite hard to use. The pain is quite apparent, we have to:\n",
    "\n",
    "1. We have to define a Relevance Evaluator\n",
    "2. Massage the dataframe\n",
    "3. compute the nDCG\n",
    "4. precision\n",
    "5. hit rate ourselves\n",
    "\n",
    "\n",
    "Generally speaking almost all this code, can be put into one \"evaluate and put on phoenix\". The API is not declarative at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from phoenix.experimental.evals import (\n",
    "    RelevanceEvaluator,\n",
    "    run_evals,\n",
    ")\n",
    "from phoenix.session.evaluation import get_retrieved_documents\n",
    "import numpy as np\n",
    "from sklearn.metrics import ndcg_score\n",
    "\n",
    "\n",
    "retrieved_documents_df = get_retrieved_documents(px.Client())\n",
    "relevance_evaluator = RelevanceEvaluator(OpenAIModel(model_name=\"gpt-4-turbo-preview\"))\n",
    "\n",
    "retrieved_documents_relevance_df = run_evals(\n",
    "    evaluators=[relevance_evaluator],\n",
    "    dataframe=retrieved_documents_df,\n",
    "    provide_explanation=True,\n",
    "    concurrency=20,\n",
    ")[0]\n",
    "\n",
    "documents_with_relevance_df = pd.concat(\n",
    "    [retrieved_documents_df, retrieved_documents_relevance_df.add_prefix(\"eval_\")], axis=1\n",
    ")\n",
    "\n",
    "def _compute_ndcg(df, n):\n",
    "    n = max(len(df), 2)\n",
    "    eval_scores = np.zeros(n)\n",
    "    doc_scores = np.zeros(n)\n",
    "    eval_scores[: len(df)] = df.eval_scores\n",
    "    doc_scores[: len(df)] = df.document_scores\n",
    "    \n",
    "    try:\n",
    "        return ndcg_score([eval_scores], [doc_scores], k=k)\n",
    "    except ValueError:\n",
    "        return np.nan\n",
    "\n",
    "ndcg_at_2 = pd.DataFrame(\n",
    "    {\"score\": documents_with_relevance_df.groupby(\"context.span_id\").apply(_compute_ndcg, k=2)}\n",
    ")\n",
    "\n",
    "precision_at_2 = pd.DataFrame(\n",
    "    {\n",
    "        \"score\": documents_with_relevance_df.groupby(\"context.span_id\").apply(\n",
    "            lambda x: x.eval_score[:2].sum(skipna=False) / 2\n",
    "        )\n",
    "    }\n",
    ")\n",
    "\n",
    "hit = pd.DataFrame(\n",
    "    {\n",
    "        \"hit\": documents_with_relevance_df.groupby(\"context.span_id\").apply(\n",
    "            lambda x: x.eval_score[:2].sum(skipna=False) > 0\n",
    "        )\n",
    "    }\n",
    ")\n",
    "\n",
    "retrievals_df = px.Client().get_spans_dataframe(\"span_kind == 'RETRIEVER'\")\n",
    "rag_evaluation_dataframe = pd.concat(\n",
    "    [\n",
    "        retrievals_df[\"attributes.input.value\"],\n",
    "        ndcg_at_2.add_prefix(\"ncdg@2_\"),\n",
    "        precision_at_2.add_prefix(\"precision@2_\"),\n",
    "        hit,\n",
    "    ],\n",
    "    axis=1,\n",
    ")\n",
    "from phoenix.trace import DocumentEvaluations, SpanEvaluations\n",
    "\n",
    "px.log_evaluations(\n",
    "    SpanEvaluations(dataframe=ndcg_at_2, eval_name=\"ndcg@2\"),\n",
    "    SpanEvaluations(dataframe=precision_at_2, eval_name=\"precision@2\"),\n",
    "    DocumentEvaluations(dataframe=retrieved_documents_relevance_df, eval_name=\"relevance\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Appendix\n",
    "\n",
    "If interested you can peek at some of the data structures below. The goal was to remove this from view, as it was obstructing the natural flow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 5"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
